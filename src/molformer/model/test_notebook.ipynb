{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'src'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmolformer\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodel\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39margs\u001b[39;00m \u001b[39mimport\u001b[39;00m get_parser\n\u001b[1;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmolformer\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtokenizer\u001b[39;00m \u001b[39mimport\u001b[39;00m MolTranBertTokenizer\n\u001b[0;32m----> 4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmolformer\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodel\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mattention\u001b[39;00m \u001b[39mimport\u001b[39;00m TestBert\n\u001b[1;32m      5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmolformer\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodel\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39margs\u001b[39;00m \u001b[39mimport\u001b[39;00m get_parser \u001b[39mas\u001b[39;00m ARGS\n\u001b[1;32m      6\u001b[0m \u001b[39m# from src.molformer.model.attention import register_attention\u001b[39;00m\n",
      "File \u001b[0;32m~/git/lamalab-org/molformer/src/molformer/model/attention.py:4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpytorch_lightning\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpl\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpytorch_lightning\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutilities\u001b[39;00m \u001b[39mimport\u001b[39;00m rank_zero_warn, rank_zero_only, seed\n\u001b[0;32m----> 4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msrc\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmolformer\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodel\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbase_bert\u001b[39;00m \u001b[39mimport\u001b[39;00m LM_Layer\n\u001b[1;32m      5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msrc\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmolformer\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodel\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mrotate_builder\u001b[39;00m \u001b[39mimport\u001b[39;00m RotateEncoderBuilder \u001b[39mas\u001b[39;00m rotate_builder\n\u001b[1;32m      6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mfast_transformers\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mevents\u001b[39;00m \u001b[39mimport\u001b[39;00m EventDispatcher, AttentionEvent\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'src'"
     ]
    }
   ],
   "source": [
    "\n",
    "from molformer.utils import get_argparse_defaults, read_config\n",
    "from molformer.model.args import get_parser\n",
    "from molformer.tokenizer import MolTranBertTokenizer\n",
    "from molformer.model.attention import TestBert\n",
    "from molformer.model.args import get_parser as ARGS\n",
    "# from src.molformer.model.attention import register_attention\n",
    "from molformer.model.attention_layer import apply_rotary_pos_emb\n",
    "from molformer.model.attention import get_full_attention\n",
    "from molformer.utils import calc_attention_tensor, connectivity_matrix, distance_matrix\n",
    "\n",
    "from attributedict.collections import AttributeDict\n",
    "import torch \n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "\n",
    "def get_bert(config, tokenizer):\n",
    "    bert_model = (\n",
    "        TestBert(\n",
    "            config,\n",
    "            tokenizer.vocab,\n",
    "            config.seed_path,\n",
    "            rotate=config.rotate,\n",
    "            device=config.device,\n",
    "        )\n",
    "        .to(config.device)\n",
    "        .eval()\n",
    "    )\n",
    "\n",
    "    tmp_model = torch.load(config.seed_path)[\"state_dict\"]\n",
    "#     print(tmp_model.keys())\n",
    "    bert_model.load_state_dict(dict(tmp_model), strict=True)\n",
    "    return bert_model\n",
    "\n",
    "### defaults\n",
    "DEFAULTS = AttributeDict(get_argparse_defaults(ARGS()))\n",
    "\n",
    "### import|model\n",
    "# register_attention(\"linear\")\n",
    "BERT = get_bert(CONFIG, TOKENIZER)\n",
    "\n",
    "def get_full_model(CHECKPOINT_PATH : str, VOCAB_PATH : str, CONFIG_PATH):\n",
    "    DEFAULTS = AttributeDict(get_argparse_defaults(ARGS()))\n",
    "    CONFIG_FILE_PATH = '/home/mi52qep/proj/molformer/data/Pretrained MoLFormer/hparams.yaml'\n",
    "    CONFIG = read_config(CONFIG_FILE_PATH)\n",
    "    \n",
    "    CONFIG.seed_path = CHECKPOINT_PATH\n",
    "    TOKENIZER = MolTranBertTokenizer(VOCAB_PATH)\n",
    "    TOKENIZER.name_or_path = VOCAB_PATH\n",
    "    BERT = get_bert(CONFIG, TOKENIZER)\n",
    "    return BERT\n",
    "\n",
    "\n",
    "def agg_attention(smiles : str, nr_of_layers, CONFIG_PATH, VOCAB_PATH):\n",
    "    for layer in range(nr_of_layers):\n",
    "        if layer == 0:\n",
    "            \n",
    "            attention_tensor = calc_attention_tensor(smiles, layer, CONFIG_PATH, VOCAB_PATH)\n",
    "        \n",
    "        else:\n",
    "            attention_tensor += calc_attention_tensor(smiles, layer, CONFIG_PATH, VOCAB_PATH)\n",
    "    \n",
    "    return attention_tensor/nr_of_layers\n",
    "\n",
    "\n",
    "def r2(x, y):\n",
    "    x = x.flatten()\n",
    "    y = y.flatten()\n",
    "    LR = LinearRegression()\n",
    "    LR.fit(np.array(x).reshape(-1, 1), y)\n",
    "    y_pred = LR.predict(np.array(x).reshape(-1, 1))\n",
    "\n",
    "    return r2_score(y_pred, y)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = '/home/mi52qep/proj/molformer_refactor/src/molformer/data/bace/test.csv'\n",
    "BACE_TEST = pd.read_csv(DATA_PATH).smiles\n",
    "CHECKPOINT_PATHS = glob.glob(\"/home/mi52qep/proj/molformer/data/Pretrained MoLFormer/checkpoints/*.ckpt\")\n",
    "VOCAB_PATH = '/home/mi52qep/proj/molformer_refactor/src/molformer/data/bert_vocab.txt'\n",
    "\n",
    "\n",
    "CONFIG_FILE_PATH = '/home/mi52qep/proj/molformer/data/Pretrained MoLFormer/hparams.yaml'\n",
    "CONFIG = read_config(CONFIG_FILE_PATH)\n",
    "CONFIG.seed_path = CHECKPOINT_PATH\n",
    "CONFIG.canonical = False\n",
    "CONFIG.mask = False\n",
    "\n",
    "TEST_SMILES = BACE_TEST[0]\n",
    "\n",
    "for CHECKPOINT_PATH in CHECKPOINT_PATHS:\n",
    "    model = get_full_model(CHECKPOINT_PATH, VOCAB_PATH, CONFIG_FILE_PATH)\n",
    "    attention = agg_attention(TEST_SMILES, 12, CONFIG, VOCAB_PATH)\n",
    "    distance_matrix = distance_matrix(TEST_SMILES)\n",
    "    r2 = r2(attention, distance_matrix)\n",
    "    %%capture\n",
    "    print(r2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "molformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
