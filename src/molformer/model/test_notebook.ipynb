{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "!python -m pip install -e ../../.\n",
    "\n",
    "from src.molformer.utils import get_argparse_defaults, read_config\n",
    "from src.molformer.model.args import get_parser\n",
    "from src.molformer.tokenizer import MolTranBertTokenizer\n",
    "from src.molformer.model.attention import TestBert\n",
    "from src.molformer.model.args import get_parser as ARGS\n",
    "# from src.molformer.model.attention import register_attention\n",
    "from src.molformer.model.attention_layer import apply_rotary_pos_emb\n",
    "from src.molformer.model.attention import get_full_attention\n",
    "from src.molformer.utils import calc_attention_tensor, connectivity_matrix, distance_matrix\n",
    "\n",
    "from attributedict.collections import AttributeDict\n",
    "import torch \n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "\n",
    "def get_bert(config, tokenizer):\n",
    "    bert_model = (\n",
    "        TestBert(\n",
    "            config,\n",
    "            tokenizer.vocab,\n",
    "            config.seed_path,\n",
    "            rotate=config.rotate,\n",
    "            device=config.device,\n",
    "        )\n",
    "        .to(config.device)\n",
    "        .eval()\n",
    "    )\n",
    "\n",
    "    tmp_model = torch.load(config.seed_path)[\"state_dict\"]\n",
    "#     print(tmp_model.keys())\n",
    "    bert_model.load_state_dict(dict(tmp_model), strict=True)\n",
    "    return bert_model\n",
    "\n",
    "### defaults\n",
    "DEFAULTS = AttributeDict(get_argparse_defaults(ARGS()))\n",
    "\n",
    "CHECKPOINT_PATH = '/home/mi52qep/proj/molformer/data/Pretrained MoLFormer/checkpoints/N-Step-Checkpoint_0_0.ckpt'\n",
    "\n",
    "CONFIG = DEFAULTS\n",
    "\n",
    "# CONFIG.attention_type = 'linear'\n",
    "    \n",
    "### read|config|file\n",
    "CONFIG_FILE_PATH = '/home/mi52qep/proj/molformer/data/Pretrained MoLFormer/hparams.yaml'\n",
    "CONFIG = read_config(CONFIG_FILE_PATH)\n",
    "CONFIG.seed_path = CHECKPOINT_PATH\n",
    "\n",
    "### here attention type is registered\n",
    "# CONFIG.attention_type = 'linear'\n",
    "CONFIG.canonical = False\n",
    "CONFIG.mask = False\n",
    "\n",
    "### tokenizer\n",
    "VOCAB_PATH = '/home/mi52qep/proj/molformer_refactor/src/molformer/data/bert_vocab.txt'\n",
    "TOKENIZER = MolTranBertTokenizer(VOCAB_PATH)\n",
    "\n",
    "### import|model\n",
    "# register_attention(\"linear\")\n",
    "BERT = get_bert(CONFIG, TOKENIZER)\n",
    "\n",
    "def get_full_model(CHECKPOINT_PATH : str, VOCAB_PATH : str, CONFIG_PATH):\n",
    "    DEFAULTS = AttributeDict(get_argparse_defaults(ARGS()))\n",
    "    CONFIG_FILE_PATH = '/home/mi52qep/proj/molformer/data/Pretrained MoLFormer/hparams.yaml'\n",
    "    CONFIG = read_config(CONFIG_FILE_PATH)\n",
    "    \n",
    "    CONFIG.seed_path = CHECKPOINT_PATH\n",
    "    TOKENIZER = MolTranBertTokenizer(VOCAB_PATH)\n",
    "    TOKENIZER.name_or_path = VOCAB_PATH\n",
    "    BERT = get_bert(CONFIG, TOKENIZER)\n",
    "    return BERT\n",
    "\n",
    "\n",
    "def agg_attention(smiles : str, nr_of_layers, CONFIG_PATH, VOCAB_PATH):\n",
    "    for layer in range(nr_of_layers):\n",
    "        if layer == 0:\n",
    "            \n",
    "            attention_tensor = calc_attention_tensor(smiles, layer, CONFIG_PATH, VOCAB_PATH)\n",
    "        \n",
    "        else:\n",
    "            attention_tensor += calc_attention_tensor(smiles, layer, CONFIG_PATH, VOCAB_PATH)\n",
    "    \n",
    "    return attention_tensor/nr_of_layers\n",
    "\n",
    "\n",
    "def r2(x, y):\n",
    "    x = x.flatten()\n",
    "    y = y.flatten()\n",
    "    LR = LinearRegression()\n",
    "    LR.fit(np.array(x).reshape(-1, 1), y)\n",
    "    y_pred = LR.predict(np.array(x).reshape(-1, 1))\n",
    "\n",
    "    return r2_score(y_pred, y)\n",
    "\n",
    "\n",
    "\n",
    "# import data\n",
    "DATA_PATH = '/home/mi52qep/proj/molformer_refactor/src/molformer/data/bace/test.csv'\n",
    "BACE_TEST = pd.read_csv(DATA_PATH).smiles\n",
    "CHECKPOINT_PATHS = glob.glob(\"/home/mi52qep/proj/molformer/data/Pretrained MoLFormer/checkpoints/*.ckpt\")\n",
    "VOCAB_PATH = '/home/mi52qep/proj/molformer_refactor/src/molformer/data/bert_vocab.txt'\n",
    "\n",
    "\n",
    "CONFIG_FILE_PATH = '/home/mi52qep/proj/molformer/data/Pretrained MoLFormer/hparams.yaml'\n",
    "CONFIG = read_config(CONFIG_FILE_PATH)\n",
    "CONFIG.seed_path = CHECKPOINT_PATH\n",
    "CONFIG.canonical = False\n",
    "CONFIG.mask = False\n",
    "\n",
    "TEST_SMILES = BACE_TEST[0]\n",
    "\n",
    "for CHECKPOINT_PATH in CHECKPOINT_PATHS:\n",
    "    model = get_full_model(CHECKPOINT_PATH, VOCAB_PATH, CONFIG_FILE_PATH)\n",
    "    attention = agg_attention(TEST_SMILES, 12, CONFIG, VOCAB_PATH)\n",
    "    distance_matrix = distance_matrix(TEST_SMILES)\n",
    "    r2 = r2(attention, distance_matrix)\n",
    "    %%capture\n",
    "    print(r2)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
