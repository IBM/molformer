{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from molformer.utils import get_argparse_defaults, read_config\n",
    "from molformer.tokenizer import MolTranBertTokenizer\n",
    "from molformer.model.attention import TestBert\n",
    "from molformer.model.args import get_parser as ARGS\n",
    "from molformer.analysis import get_full_attention\n",
    "from attributedict.collections import AttributeDict\n",
    "import torch \n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "\n",
    "def get_bert(config, tokenizer):\n",
    "    bert_model = (\n",
    "        TestBert(\n",
    "            tokenizer.vocab,\n",
    "            config.seed_path,\n",
    "            rotate=config.rotate,\n",
    "            device=config.device,\n",
    "        )\n",
    "        .to(config.device)\n",
    "        .eval()\n",
    "    )\n",
    "    bert_model.load_state_dict(dict(torch.load(config.seed_path, map_location=config.device)[\"state_dict\"]), strict=True )\n",
    "    return bert_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Rotation Embedding\n",
      "Using Rotation Embedding\n",
      "Using Rotation Embedding\n",
      "Using Rotation Embedding\n",
      "Using Rotation Embedding\n",
      "Using Rotation Embedding\n",
      "Using Rotation Embedding\n",
      "Using Rotation Embedding\n",
      "Using Rotation Embedding\n",
      "Using Rotation Embedding\n",
      "Using Rotation Embedding\n",
      "Using Rotation Embedding\n"
     ]
    }
   ],
   "source": [
    "\n",
    "### defaults\n",
    "DEFAULTS = AttributeDict(get_argparse_defaults(ARGS()))\n",
    "TOKENIZER = MolTranBertTokenizer(\"finetune/bert_vocab.txt\")\n",
    "DEFAULTS.device = 'cpu'\n",
    "DEFAULTS.seed_path = '/Users/kevinmaikjablonka/Downloads/Pretrained MoLFormer/checkpoints/N-Step-Checkpoint_0_0.ckpt' # Doing this is kind of ugly. In the medium to long run this should be a real configuration system (e.g. Hydra)\n",
    "### import|model\n",
    "# register_attention(\"linear\")\n",
    "model = get_bert(DEFAULTS, TOKENIZER)\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = TOKENIZER(\"CCO\", return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute '_mask'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model(tokenized[\u001b[39m\"\u001b[39;49m\u001b[39minput_ids\u001b[39;49m\u001b[39m\"\u001b[39;49m], mask\u001b[39m=\u001b[39;49mtokenized[\u001b[39m\"\u001b[39;49m\u001b[39mattention_mask\u001b[39;49m\u001b[39m\"\u001b[39;49m], mode\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mfull\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m~/miniconda3/envs/molformer/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/git/lamalab-org/molformer/src/molformer/model/attention.py:99\u001b[0m, in \u001b[0;36mTestBert.forward\u001b[0;34m(self, batch, mask, mode)\u001b[0m\n\u001b[1;32m     95\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdrop(token_embeddings)\n\u001b[1;32m     97\u001b[0m \u001b[39mif\u001b[39;00m mask \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     98\u001b[0m     x, attention_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mblocks(\n\u001b[0;32m---> 99\u001b[0m         x, length_mask\u001b[39m=\u001b[39mLengthMask(mask\u001b[39m.\u001b[39;49m_mask\u001b[39m.\u001b[39msum(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m))\n\u001b[1;32m    100\u001b[0m     )\n\u001b[1;32m    102\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    103\u001b[0m     x, attention_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mblocks(x)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Tensor' object has no attribute '_mask'"
     ]
    }
   ],
   "source": [
    "model(tokenized[\"input_ids\"], mask=tokenized[\"attention_mask\"], mode=\"full\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'view'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m get_full_attention(\u001b[39m\"\u001b[39;49m\u001b[39mCCO\u001b[39;49m\u001b[39m\"\u001b[39;49m, model, DEFAULTS, TOKENIZER)\n",
      "File \u001b[0;32m~/git/lamalab-org/molformer/src/molformer/analysis.py:97\u001b[0m, in \u001b[0;36mget_full_attention\u001b[0;34m(molecule, bert_model, config, tokenizer)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     90\u001b[0m     att_mask \u001b[39m=\u001b[39m FullMask(\n\u001b[1;32m     91\u001b[0m         torch\u001b[39m.\u001b[39mones(\n\u001b[1;32m     92\u001b[0m             torch\u001b[39m.\u001b[39mtensor(batch_ids[\u001b[39m\"\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m\"\u001b[39m])\u001b[39m.\u001b[39msize(), dtype\u001b[39m=\u001b[39m\u001b[39mbool\u001b[39m\n\u001b[1;32m     93\u001b[0m         )\u001b[39m.\u001b[39mto(device),\n\u001b[1;32m     94\u001b[0m         device\u001b[39m=\u001b[39mdevice,\n\u001b[1;32m     95\u001b[0m     )\n\u001b[0;32m---> 97\u001b[0m embeddings, attention_mask \u001b[39m=\u001b[39m bert_model(\n\u001b[1;32m     98\u001b[0m     torch\u001b[39m.\u001b[39;49mtensor(batch_ids[\u001b[39m\"\u001b[39;49m\u001b[39minput_ids\u001b[39;49m\u001b[39m\"\u001b[39;49m])\u001b[39m.\u001b[39;49mto(device),\n\u001b[1;32m     99\u001b[0m     att_mask,\n\u001b[1;32m    100\u001b[0m     mode\u001b[39m=\u001b[39;49mconfig\u001b[39m.\u001b[39;49mmode,\n\u001b[1;32m    101\u001b[0m )\n\u001b[1;32m    102\u001b[0m \u001b[39mreturn\u001b[39;00m attention_mask, raw_tokens\n",
      "File \u001b[0;32m~/miniconda3/envs/molformer/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/git/lamalab-org/molformer/src/molformer/model/attention.py:115\u001b[0m, in \u001b[0;36mTestBert.forward\u001b[0;34m(self, batch, mask, mode)\u001b[0m\n\u001b[1;32m    112\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdrop(token_embeddings)\n\u001b[1;32m    114\u001b[0m \u001b[39mif\u001b[39;00m mask \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 115\u001b[0m     x, attention_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mblocks(\n\u001b[1;32m    116\u001b[0m         x, length_mask\u001b[39m=\u001b[39;49mLengthMask(mask\u001b[39m.\u001b[39;49m_mask\u001b[39m.\u001b[39;49msum(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m))\n\u001b[1;32m    117\u001b[0m     )\n\u001b[1;32m    119\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    120\u001b[0m     x, attention_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mblocks(x)\n",
      "File \u001b[0;32m~/miniconda3/envs/molformer/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/molformer/lib/python3.8/site-packages/fast_transformers/transformers.py:138\u001b[0m, in \u001b[0;36mTransformerEncoder.forward\u001b[0;34m(self, x, attn_mask, length_mask)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[39m# Apply all the transformers\u001b[39;00m\n\u001b[1;32m    137\u001b[0m \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers:\n\u001b[0;32m--> 138\u001b[0m     x \u001b[39m=\u001b[39m layer(x, attn_mask\u001b[39m=\u001b[39;49mattn_mask, length_mask\u001b[39m=\u001b[39;49mlength_mask)\n\u001b[1;32m    140\u001b[0m \u001b[39m# Apply the normalization if needed\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/molformer/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/molformer/lib/python3.8/site-packages/fast_transformers/transformers.py:77\u001b[0m, in \u001b[0;36mTransformerEncoderLayer.forward\u001b[0;34m(self, x, attn_mask, length_mask)\u001b[0m\n\u001b[1;32m     73\u001b[0m length_mask \u001b[39m=\u001b[39m length_mask \u001b[39mor\u001b[39;00m \\\n\u001b[1;32m     74\u001b[0m     LengthMask(x\u001b[39m.\u001b[39mnew_full((N,), L, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mint64))\n\u001b[1;32m     76\u001b[0m \u001b[39m# Run self attention and add it to the input\u001b[39;00m\n\u001b[0;32m---> 77\u001b[0m x \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention(\n\u001b[1;32m     78\u001b[0m     x, x, x,\n\u001b[1;32m     79\u001b[0m     attn_mask\u001b[39m=\u001b[39;49mattn_mask,\n\u001b[1;32m     80\u001b[0m     query_lengths\u001b[39m=\u001b[39;49mlength_mask,\n\u001b[1;32m     81\u001b[0m     key_lengths\u001b[39m=\u001b[39;49mlength_mask\n\u001b[1;32m     82\u001b[0m ))\n\u001b[1;32m     84\u001b[0m \u001b[39m# Run the fully connected part of the layer\u001b[39;00m\n\u001b[1;32m     85\u001b[0m y \u001b[39m=\u001b[39m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm1(x)\n",
      "File \u001b[0;32m~/miniconda3/envs/molformer/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/git/lamalab-org/molformer/src/molformer/model/attention_modules/attention_layer.py:84\u001b[0m, in \u001b[0;36mRotateAttentionLayer.forward\u001b[0;34m(self, queries, keys, values, attn_mask, query_lengths, key_lengths)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mevent_dispatcher\u001b[39m.\u001b[39mdispatch(QKVEvent(\u001b[39mself\u001b[39m, queries, keys, values))\n\u001b[1;32m     83\u001b[0m \u001b[39m# Compute the attention\u001b[39;00m\n\u001b[0;32m---> 84\u001b[0m new_values \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minner_attention(\n\u001b[1;32m     85\u001b[0m     queries, keys, values, attn_mask, query_lengths, key_lengths\n\u001b[1;32m     86\u001b[0m )\u001b[39m.\u001b[39;49mview(N, L, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     88\u001b[0m \u001b[39m# Project the output and return\u001b[39;00m\n\u001b[1;32m     89\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mout_projection(new_values)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'view'"
     ]
    }
   ],
   "source": [
    "get_full_attention(\"CCO\", model, DEFAULTS, TOKENIZER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = '/home/mi52qep/proj/molformer_refactor/src/molformer/data/bace/test.csv'\n",
    "BACE_TEST = pd.read_csv(DATA_PATH).smiles\n",
    "CHECKPOINT_PATHS = glob.glob(\"/home/mi52qep/proj/molformer/data/Pretrained MoLFormer/checkpoints/*.ckpt\")\n",
    "VOCAB_PATH = '/home/mi52qep/proj/molformer_refactor/src/molformer/data/bert_vocab.txt'\n",
    "\n",
    "\n",
    "CONFIG_FILE_PATH = '/home/mi52qep/proj/molformer/data/Pretrained MoLFormer/hparams.yaml'\n",
    "CONFIG = read_config(CONFIG_FILE_PATH)\n",
    "CONFIG.seed_path = CHECKPOINT_PATH\n",
    "CONFIG.canonical = False\n",
    "CONFIG.mask = False\n",
    "\n",
    "TEST_SMILES = BACE_TEST[0]\n",
    "\n",
    "for CHECKPOINT_PATH in CHECKPOINT_PATHS:\n",
    "    model = get_full_model(CHECKPOINT_PATH, VOCAB_PATH, CONFIG_FILE_PATH)\n",
    "    attention = agg_attention(TEST_SMILES, 12, CONFIG, VOCAB_PATH)\n",
    "    distance_matrix = distance_matrix(TEST_SMILES)\n",
    "    r2 = r2(attention, distance_matrix)\n",
    "    %%capture\n",
    "    print(r2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "molformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
